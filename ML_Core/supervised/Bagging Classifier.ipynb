{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cafd1422",
   "metadata": {},
   "source": [
    "# Bagging Classifier \n",
    "\n",
    "\n",
    "\n",
    "Bagging (Bootstrap Aggregating) is an ensemble learning technique that improves model stability and accuracy by combining multiple base estimators trained on random subsets of the data.\n",
    "\n",
    "Train multiple instances of the same algorithm on different data samples, then combine their predictions.\n",
    "\n",
    "Bootstrap Sampling\n",
    "Creates multiple datasets by sampling with replacement from the original training data\n",
    "\n",
    "Each bootstrap sample contains ~63.2% unique instances (on average)\n",
    "\n",
    "The remaining ~36.8% are \"out-of-bag\" (OOB) samples that can be used for validation\n",
    "\n",
    "Prediction Aggregation\n",
    "Classification: Majority voting (hard voting) or average probabilities (soft voting)\n",
    "\n",
    "Regression: Average of predictions\n",
    "\n",
    "Mathematical Formulation:\n",
    "For classification with n_estimators:\n",
    "\n",
    "``` ŷ = mode{h₁(x), h₂(x), ..., h_n(x)}```\n",
    "For regression:\n",
    "\n",
    "```ŷ = (1/n) * Σ h_i(x)```\n",
    "\n",
    "\n",
    "### Types of Bagging Classifiers\n",
    "\n",
    "```bash\n",
    "1. Standard BaggingClassifier\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Base estimator\n",
    "base_estimator = DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "# Bagging classifier\n",
    "bagging = BaggingClassifier(\n",
    "    estimator=base_estimator,\n",
    "    n_estimators=100,\n",
    "    max_samples=0.8,\n",
    "    max_features=0.8,\n",
    "    bootstrap=True,\n",
    "    bootstrap_features=False,\n",
    "    oob_score=True,\n",
    "    random_state=42\n",
    ")\n",
    "2. Random Forest (Specialized Bagging)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Random Forest is essentially bagging with decision trees\n",
    "# with additional feature randomization at each split\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    max_features='sqrt',  # Feature bagging\n",
    "    bootstrap=True,\n",
    "    oob_score=True,\n",
    "    random_state=42\n",
    ")\n",
    "3. Extra-Trees Classifier (Extremely Randomized Trees)\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# Similar to Random Forest but with random splits\n",
    "et = ExtraTreesClassifier(\n",
    "    n_estimators=100,\n",
    "    max_features='sqrt',\n",
    "    bootstrap=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "## Essential Parameters\n",
    "n_estimators: Number of base estimators (more = less variance, but diminishing returns)\n",
    "\n",
    "max_samples: Fraction/number of samples for each bootstrap (default=1.0)\n",
    "\n",
    "max_features: Fraction/number of features for each estimator\n",
    "\n",
    "bootstrap: Whether to sample with replacement (True for bagging)\n",
    "\n",
    "bootstrap_features: Whether to sample features with replacement\n",
    "\n",
    "oob_score: Whether to use out-of-bag samples for validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600cd99d",
   "metadata": {},
   "source": [
    "## How does Bagging reduce overfitting?\n",
    "\n",
    "Bagging reduces overfitting by averaging multiple models trained on different data subsets. This decreases variance without increasing bias. The randomness in sampling ensures models learn different patterns, and their aggregation cancels out individual errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c99aea",
   "metadata": {},
   "source": [
    "### Strengths & Weaknesses\n",
    "Advantages of Bagging\n",
    "Reduces Variance: Especially effective for high-variance estimators (deep decision trees)\n",
    "\n",
    "Improves Stability: Less sensitive to noise and outliers\n",
    "\n",
    "Parallelizable: Estimators can be trained independently\n",
    "\n",
    "OOB Validation: Built-in validation without separate validation set\n",
    "\n",
    "Handles Overfitting: Can regularize complex base estimators\n",
    "\n",
    "Disadvantages\n",
    "Computationally Expensive: Requires training multiple models\n",
    "\n",
    "Less Interpretable: Harder to explain than single models\n",
    "\n",
    "Memory Intensive: Stores multiple models\n",
    "\n",
    "May Not Reduce Bias: If base estimator is biased, bagging won't help\n",
    "\n",
    "Can Underfit: If base estimator is too simple\n",
    "\n",
    "When to Use Bagging\n",
    " Base estimator has high variance (e.g., deep decision trees)\n",
    "\n",
    " You have sufficient computational resources\n",
    "\n",
    " Model stability is important\n",
    "\n",
    " Parallel computing is available\n",
    "\n",
    "When to Avoid Bagging\n",
    " Base estimator is already low-variance (e.g., linear models)\n",
    "\n",
    " Computational resources are limited\n",
    "\n",
    " Model interpretability is crucial\n",
    "\n",
    " Training data is very small"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
