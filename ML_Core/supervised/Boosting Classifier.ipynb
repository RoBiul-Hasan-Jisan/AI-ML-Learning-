{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5d6a6b5",
   "metadata": {},
   "source": [
    "#  Boosting\n",
    "\n",
    "\n",
    "Boosting is an ensemble learning technique that combines multiple weak learners (slightly better than random guessing) into a strong learner through sequential training with error correction.\n",
    "\n",
    " Each new model focuses on the mistakes made by previous models, gradually improving overall performance.\n",
    "\n",
    "How Boosting Works\n",
    "Train initial model on the original dataset\n",
    "\n",
    "Identify misclassified instances\n",
    "\n",
    "Increase weights of misclassified instances\n",
    "\n",
    "Train next model on reweighted data\n",
    "\n",
    "Combine models with appropriate weights\n",
    "\n",
    "Repeat until stopping criteria met\n",
    "\n",
    "Mathematical Foundation\n",
    "For AdaBoost (discrete boosting):\n",
    "\n",
    "\n",
    "``` Final model: H(x) = sign(∑ α_t * h_t(x))```\n",
    "where α_t = 0.5 * ln((1 - ε_t) / ε_t) is the weight of classifier h_t with error ε_t\n",
    "\n",
    "For Gradient Boosting:\n",
    "\n",
    "\n",
    "``` F_m(x) = F_{m-1}(x) + γ_m * h_m(x)```\n",
    "where h_m(x) fits the negative gradient of the loss function\n",
    "\n",
    "## Types of Boosting Algorithms\n",
    "```bash\n",
    "1. AdaBoost (Adaptive Boosting)\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# AdaBoost with decision stumps (max_depth=1)\n",
    "ada = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=100,\n",
    "    learning_rate=1.0,\n",
    "    algorithm='SAMME.R',  # or 'SAMME'\n",
    "    random_state=42\n",
    ")\n",
    "2. Gradient Boosting Machines (GBM)\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbm = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    subsample=1.0,  # Stochastic gradient boosting\n",
    "    max_features=None,\n",
    "    random_state=42\n",
    ")\n",
    "3. XGBoost (Extreme Gradient Boosting)\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    min_child_weight=1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0,  # L1 regularization\n",
    "    reg_lambda=1,  # L2 regularization\n",
    "    random_state=42\n",
    ")\n",
    "4. LightGBM\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    num_leaves=31,\n",
    "    max_depth=-1,\n",
    "    min_child_samples=20,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.0,\n",
    "    reg_lambda=0.0,\n",
    "    random_state=42\n",
    ")\n",
    "5. CatBoost\n",
    "\n",
    "import catboost as cb\n",
    "\n",
    "cat_model = cb.CatBoostClassifier(\n",
    "    iterations=100,\n",
    "    learning_rate=0.1,\n",
    "    depth=6,\n",
    "    l2_leaf_reg=3,\n",
    "    random_seed=42,\n",
    "    verbose=False  # Set to True for training progress\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf04834",
   "metadata": {},
   "source": [
    "## Key Parameters Explained\n",
    "1. Core Boosting Parameters\n",
    "n_estimators: Number of boosting stages (more = better but risk overfitting)\n",
    "\n",
    "learning_rate: Shrinks contribution of each tree (lower = more trees needed)\n",
    "\n",
    "base_estimator: Weak learner (decision stump for AdaBoost)\n",
    "\n",
    "2. Tree-Specific Parameters\n",
    "max_depth: Limits tree depth (controls complexity)\n",
    "\n",
    "min_samples_split: Minimum samples to split a node\n",
    "\n",
    "min_samples_leaf: Minimum samples in a leaf node\n",
    "\n",
    "subsample: Fraction of samples for each tree (stochastic boosting)\n",
    "\n",
    "max_features: Number/percentage of features for each split\n",
    "\n",
    "3. Regularization Parameters\n",
    "reg_alpha: L1 regularization (XGBoost, LightGBM)\n",
    "\n",
    "reg_lambda: L2 regularization (XGBoost, LightGBM)\n",
    "\n",
    "min_child_weight: Minimum sum of instance weight needed in child (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0d1753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost specific\n",
    "ada_params = {\n",
    "    'algorithm': 'SAMME.R',  # Real boosting (requires predict_proba)\n",
    "    'estimator': DecisionTreeClassifier(max_depth=1)  # Decision stump\n",
    "}\n",
    "\n",
    "# XGBoost specific\n",
    "xgb_params = {\n",
    "    'booster': 'gbtree',  # or 'gblinear', 'dart'\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'gamma': 0,  # Minimum loss reduction for split\n",
    "    'scale_pos_weight': sum(y == 0) / sum(y == 1)  # Handle imbalance\n",
    "}\n",
    "\n",
    "# LightGBM specific\n",
    "lgb_params = {\n",
    "    'boosting_type': 'gbdt',  # 'dart', 'goss', 'rf'\n",
    "    'num_leaves': 31,  # Main parameter to control complexity\n",
    "    'min_data_in_leaf': 20,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79ecf7a",
   "metadata": {},
   "source": [
    "### Strengths & Weaknesses\n",
    "Advantages of Boosting\n",
    "High Accuracy: Often achieves state-of-the-art results\n",
    "\n",
    "Handles Complex Patterns: Can model non-linear relationships\n",
    "\n",
    "Feature Importance: Provides interpretable feature rankings\n",
    "\n",
    "Handles Mixed Data: Works with numerical and categorical features\n",
    "\n",
    "Built-in Regularization: Modern implementations prevent overfitting\n",
    "\n",
    "Handles Missing Values: Some implementations (XGBoost, LightGBM)\n",
    "\n",
    "Disadvantages\n",
    "Computationally Intensive: Sequential training can be slow\n",
    "\n",
    "Hyperparameter Sensitivity: Requires careful tuning\n",
    "\n",
    "Risk of Overfitting: Without proper regularization\n",
    "\n",
    "Less Interpretable: Complex ensembles harder to explain\n",
    "\n",
    "Memory Usage: Stores many trees\n",
    "\n",
    "Sensitive to Noise: Can overfit to outliers\n",
    "\n",
    "When to Use Boosting\n",
    " Tabular data with mixed feature types\n",
    "\n",
    " High accuracy is primary goal\n",
    "\n",
    " Feature importance interpretation needed\n",
    "\n",
    " Competitions (Kaggle, etc.)\n",
    "\n",
    " Imbalanced datasets (with proper handling)\n",
    "\n",
    "When to Avoid Boosting\n",
    " Very large datasets (consider linear models)\n",
    "\n",
    " Interpretability is critical (use simpler models)\n",
    "\n",
    " Limited computational resources\n",
    "\n",
    " High-dimensional sparse data (consider linear models)\n",
    "\n",
    " Online learning requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09aaf66",
   "metadata": {},
   "source": [
    "### Real-World Applications\n",
    "1. Financial Risk Modeling\n",
    "```bash\n",
    "# Credit risk assessment with XGBoost\n",
    "xgb_risk = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='auc',\n",
    "    scale_pos_weight=10,  # Handle rare fraud cases\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=500,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=42\n",
    ")\n",
    "```\n",
    "2. Medical Diagnosis Systems\n",
    "Disease prediction from patient records\n",
    "\n",
    "Medical image analysis\n",
    "\n",
    "Drug discovery and toxicity prediction\n",
    "\n",
    "3. Recommendation Engines\n",
    "Product recommendations (Amazon, Netflix)\n",
    "\n",
    "Content personalization\n",
    "\n",
    "Customer churn prediction\n",
    "\n",
    "4. Anomaly Detection\n",
    "Fraud detection in transactions\n",
    "\n",
    "Network intrusion detection\n",
    "\n",
    "Manufacturing defect detection\n",
    "\n",
    "5. Natural Language Processing\n",
    "Sentiment analysis\n",
    "\n",
    "Text classification\n",
    "\n",
    "Named entity recognition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9ddd9e",
   "metadata": {},
   "source": [
    "Explain the fundamental difference between Bagging and Boosting\n",
    "\n",
    "Bagging trains models in parallel on bootstrap samples and averages predictions (focus on variance reduction). Boosting trains models sequentially where each new model focuses on errors of previous ones (focus on bias reduction). Bagging uses equal weights, boosting weights models by performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10893492",
   "metadata": {},
   "source": [
    "Why does Boosting often outperform other algorithms?\n",
    "\n",
    "*1. Sequential error correction: Each model learns from previous mistakes\n",
    "2. Focus on hard examples: Misclassified instances get higher weights\n",
    "3. Creates complex decision boundaries: Through additive modeling\n",
    "4. Adaptive learning rate: Controls contribution of each weak learner\n",
    "5. Built-in regularization: Modern implementations prevent overfitting*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9662303",
   "metadata": {},
   "source": [
    "What's the risk of using too many estimators in Boosting?\n",
    "\n",
    "*1. Overfitting: Models memorize noise in training data\n",
    "2. Diminishing returns: Little improvement after certain point\n",
    "3. Increased computation: Training and prediction time grows\n",
    "4. Model complexity: Harder to interpret and explain\n",
    "Solution: Use early stopping and cross-validation*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a56c855",
   "metadata": {},
   "source": [
    "When would you choose XGBoost over LightGBM or vice versa?\n",
    "\n",
    "*Choose XGBoost when:\n",
    "\n",
    "Smaller datasets (<100K rows)\n",
    "\n",
    "Need extensive hyperparameter tuning\n",
    "\n",
    "Want better default performance\n",
    "\n",
    "Need proven reliability\n",
    "Choose LightGBM when:\n",
    "\n",
    "Very large datasets (>100K rows)\n",
    "\n",
    "Need faster training\n",
    "\n",
    "Have many categorical features\n",
    "\n",
    "Memory constraints exist*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a761af2",
   "metadata": {},
   "source": [
    "Parameter Tuning Priority\n",
    "Learning rate and n_estimators (most important)\n",
    "\n",
    "Tree depth (max_depth, num_leaves)\n",
    "\n",
    "Regularization (reg_alpha, reg_lambda)\n",
    "\n",
    "Sampling (subsample, colsample_bytree)\n",
    "\n",
    "Tree-specific (min_child_weight, min_samples_leaf)\n",
    "\n",
    "Common Pitfalls & Solutions\n",
    "Overfitting: Reduce max_depth, increase regularization, use early stopping\n",
    "\n",
    "Slow training: Reduce n_estimators, increase learning rate, use LightGBM\n",
    "\n",
    "Poor performance on imbalanced data: Adjust class weights, use scale_pos_weight\n",
    "\n",
    "High memory usage: Reduce n_estimators, use histogram-based algorithms\n",
    "\n",
    "Instability: Increase random_state, use more estimators with lower learning rate"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
