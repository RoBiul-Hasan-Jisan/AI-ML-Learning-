{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dbb70d7",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "A Decision Tree is a versatile, non-parametric supervised learning algorithm used for both classification and regression tasks. Its model forms a tree-like structure, mimicking human decision-making processes.\n",
    "\n",
    "\n",
    "### Core Components\n",
    "\n",
    "Root Node: The topmost node, representing the entire dataset. It gets split into two or more homogeneous sets.\n",
    "\n",
    "Internal Nodes (Decision Nodes): Nodes that represent a decision (or test) on a specific feature, splitting into further nodes.\n",
    "\n",
    "Leaf Nodes (Terminal Nodes): The final output nodes. They represent a class label (in classification) or a continuous value (in regression) and do not split further.\n",
    "\n",
    "Branches (Sub-Trees): Sections of the tree that are not the root or a leaf.\n",
    "\n",
    "## How Does a Decision Tree Learn?\n",
    "\n",
    "The algorithm learns by recursively splitting the data into purer subsets based on the features. At each node, it selects the feature and split point that best separates the data according to the target variable. This process continues until a stopping criterion is met (e.g., maximum depth is reached, or a node is 100% pure).\n",
    "\n",
    "Key Splitting Criteria\n",
    "The \"best\" split is determined by a metric that quantifies the impurity or disorder of a node.\n",
    "\n",
    "1. Gini Impurity\n",
    "Measures the probability of misclassifying a randomly chosen element from the node if it were randomly labeled according to the class distribution in the node.\n",
    "\n",
    "Formula: Gini = 1 - Σ(p_i)² where p_i is the proportion of class i in the node.\n",
    "\n",
    "Range: 0 (perfectly pure) to 0.5 (for a binary class with equal distribution).\n",
    "\n",
    "Characteristics: Computationally efficient as it doesn't involve logarithms.\n",
    "\n",
    "2. Entropy & Information Gain\n",
    "\n",
    "Entropy measures the amount of uncertainty or disorder in a node.\n",
    "\n",
    "Formula: Entropy = -Σ(p_i * log2(p_i))\n",
    "\n",
    "Range: 0 (perfectly pure) to 1 (maximally impure for binary classification).\n",
    "\n",
    "Information Gain (IG) is the reduction in entropy after a dataset is split on an attribute. The split with the highest IG is chosen.\n",
    "\n",
    "Formula: IG = Entropy(parent) - [Weighted Average] * Entropy(children)\n",
    "\n",
    "\n",
    "Gini vs. Entropy: Which to Use?\n",
    "\n",
    "Gini Impurity is slightly faster to compute and is the default in many libraries (like scikit-learn).\n",
    "\n",
    "Entropy may lead to more balanced trees as it tends to create slightly more granular splits.\n",
    "\n",
    "In practice, the difference is often negligible; the resulting trees are usually very similar.\n",
    "\n",
    "Controlling Overfitting: Pruning\n",
    "Decision trees have a tendency to learn the training data too well, including its noise. This is called overfitting. Pruning is the primary technique to combat this.\n",
    "\n",
    "Pre-Pruning (Early Stopping): Stops the tree from growing during its construction.\n",
    "\n",
    "Parameters: max_depth, min_samples_split, min_samples_leaf, min_impurity_decrease.\n",
    "\n",
    "Post-Pruning (Cost-Complexity Pruning): Grows the full tree first and then removes branches that provide little power in predicting the target variable. This is often more effective as it considers the overall performance.\n",
    "\n",
    "A hyperparameter ccp_alpha controls the trade-off between tree complexity and fit.\n",
    "\n",
    "\n",
    "#### Where Are Decision Trees Used?\n",
    "Finance: Credit scoring, loan approval.\n",
    "\n",
    "Healthcare: Disease diagnosis, patient risk stratification.\n",
    "\n",
    "E-commerce: Customer segmentation, product recommendation engines.\n",
    "\n",
    "Manufacturing: Quality control and fault detection.\n",
    "\n",
    "#### Practical Challenges & Solutions\n",
    "Handling Missing Values:\n",
    "\n",
    "Imputation: Fill with mean, median, or mode.\n",
    "\n",
    "Surrogate Splits: (C4.5, CART) Use other features that mimic the original split to handle data points with missing values.\n",
    "\n",
    "Imbalanced Data:\n",
    "\n",
    "Use class_weight parameter to assign higher weights to minority classes.\n",
    "\n",
    "Employ sampling techniques (SMOTE, undersampling) before training.\n",
    "\n",
    "Feature Types:\n",
    "\n",
    "Continuous: Find an optimal threshold (e.g., Age <= 30).\n",
    "\n",
    "Categorical: Can be handled natively by some algorithms (C4.5) or require encoding (like Ordinal/Label Encoding) for others (CART).\n",
    "\n",
    "\n",
    "### System Design Angle: Production Considerations\n",
    "When to Use Decision Trees?\n",
    "Advantages:\n",
    "\n",
    "Highly interpretable and visualizable (White Box model).\n",
    "\n",
    "Can handle both numerical and categorical data without scaling.\n",
    "\n",
    "Mirrors human decision-making, which is great for business logic.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "Prone to Overfitting: Requires careful tuning and pruning.\n",
    "\n",
    "Unstable: Small changes in data can lead to a completely different tree (high variance).\n",
    "\n",
    "Biased: Tend to favor features with more levels (e.g., continuous features).\n",
    "\n",
    "### Challenges in Production\n",
    "Model Drift: The tree's rules can become obsolete as data distributions change, requiring periodic retraining.\n",
    "\n",
    "Scalability: While fast to predict, training a very deep tree on large datasets can be memory-intensive.\n",
    "\n",
    "Instability: A single tree in production is risky. This is why ensemble methods (Random Forests, Gradient Boosting) are almost always preferred for production systems.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. How do Decision Trees handle categorical vs. continuous features?\n",
    "\n",
    "Continuous Features: The algorithm sorts the feature values and evaluates all possible split points (e.g., X > 5.2). It chooses the threshold that minimizes impurity (maximizes information gain).\n",
    "\n",
    "Categorical Features:\n",
    "\n",
    "For binary trees (like CART), it tries all possible binary partitions of the categories (e.g., Color in {Red, Blue} vs. Color in {Green}).\n",
    "\n",
    "For multi-way trees (like ID3), it can create a branch for each category.\n",
    "\n",
    "In libraries like scikit-learn, categorical features often need to be numerically encoded (e.g., LabelEncoder) beforehand.\n",
    "\n",
    "2. What is the role of entropy and information gain?\n",
    "\n",
    "Entropy quantifies the disorder within a node. A pure node has an entropy of 0.\n",
    "\n",
    "Information Gain measures how much a split reduces this entropy. The algorithm greedily selects the feature and split point that results in the highest information gain at each step, effectively creating the most homogeneous child nodes.\n",
    "\n",
    "3. Why might a Decision Tree overfit, and how can you prevent it?\n",
    "\n",
    "Why? The tree can keep splitting until every leaf node is perfectly pure, effectively memorizing the training data, including noise and outliers.\n",
    "\n",
    "Prevention:\n",
    "\n",
    "Pruning: Use cost-complexity pruning (ccp_alpha).\n",
    "\n",
    "Limit Tree Size: Set a max_depth, min_samples_split, or min_samples_leaf.\n",
    "\n",
    "Use Ensemble Methods: Train multiple trees (e.g., Random Forest) to average out the instability.\n",
    "\n",
    "Use More Data: Helps the model learn general patterns instead of noise.\n",
    "\n",
    "4. Advantages and disadvantages of Gini Impurity vs. Entropy?\n",
    "\n",
    "Criterion\tAdvantages\tDisadvantages\n",
    "Gini Impurity\tFaster to compute (no logarithms).\tTends to isolate the most frequent class in its own branch.\n",
    "Entropy\tMore theoretically grounded. May create more balanced trees.\tSlightly slower computation due to logarithms.\n",
    "In practice, they produce very similar results, and the choice is often a matter of preference.\n",
    "\n",
    "5. How do Decision Trees handle missing values?\n",
    "\n",
    "Imputation: Before training, fill missing values with a statistic like the mean or mode.\n",
    "\n",
    "Surrogate Splits (CART): A powerful technique where the tree finds a \"backup\" feature that produces a split most similar to the primary split. If the primary feature is missing for a data point, the surrogate feature is used.\n",
    "\n",
    "Ignore: Some implementations simply skip data points with missing values in the feature being considered for a split.\n",
    "\n",
    "6. Why are Decision Trees prone to instability?\n",
    "They are high-variance estimators. Because of their hierarchical nature, a small change in the training data can lead to a completely different choice at the root node, which then propagates down and changes the entire structure of the tree. This instability is why they are rarely used alone in practice.\n",
    "\n",
    "7. A Decision Tree overfits: performs well on training data but poorly on test data. What do you do?\n",
    "\n",
    "Apply Pruning: This is the most direct method. Use ccp_alpha for post-pruning.\n",
    "\n",
    "Increase Regularization: Increase min_samples_leaf or min_samples_split, or reduce max_depth.\n",
    "\n",
    "Gather More Training Data.\n",
    "\n",
    "Perform Feature Selection: Remove irrelevant features that may be adding noise.\n",
    "\n",
    "Switch to an Ensemble Method: Use a Random Forest or Gradient Boosting machine, which are built to overcome the limitations of a single tree.\n",
    "\n",
    "8. How does pruning work, and what are the different types?\n",
    "\n",
    "Pre-Pruning (Early Stopping): Stops the tree from growing based on predefined conditions (e.g., max_depth=5). It's simple but can be short-sighted.\n",
    "\n",
    "Post-Pruning (Cost-Complexity Pruning):\n",
    "\n",
    "Grow the tree to its full depth.\n",
    "\n",
    "Systematically remove branches (sub-trees) from the fully grown tree.\n",
    "\n",
    "Replace a sub-tree with a leaf node.\n",
    "\n",
    "Evaluate the performance of the pruned tree on a validation set.\n",
    "\n",
    "Select the pruned tree that maximizes validation performance. It uses a hyperparameter alpha to balance tree complexity and accuracy.\n",
    "\n",
    "9. Do Decision Trees require feature scaling?\n",
    "No. Since the splitting logic is based on ordering feature values and calculating impurity, the scale of the features (e.g., Age vs. Salary) does not influence the model. This is a significant advantage over distance-based models like SVMs or K-Nearest Neighbors.\n",
    "\n",
    "10. How do Decision Trees determine feature importance?\n",
    "Feature importance is calculated as the (normalized) total reduction of the impurity criterion (Gini/Entropy/MSE) brought by that feature.\n",
    "\n",
    "Calculation: For each feature, sum the impurity decrease for every node that splits on that feature, weighted by the fraction of samples it handles.\n",
    "\n",
    "Limitations:\n",
    "\n",
    "Bias towards high-cardinality features: Continuous features or categorical features with many levels have more split options and can appear more important.\n",
    "\n",
    "Correlation: If two features are highly correlated, the importance may be arbitrarily assigned to one, making the other seem less important.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4620985a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.0496, R²: 0.8916\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.sort(5 * np.random.rand(80, 1), axis=0)\n",
    "y = np.sin(X).ravel() + np.random.normal(0, 0.1, X.shape[0])\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train model\n",
    "tree_reg = DecisionTreeRegressor(\n",
    "    max_depth=3,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42\n",
    ")\n",
    "tree_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = tree_reg.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"MSE: {mse:.4f}, R²: {r2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
