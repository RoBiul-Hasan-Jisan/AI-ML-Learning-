{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2d3ab41",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "Naive Bayes is based on Bayes' Theorem with the \"naive\" assumption that all features are conditionally independent given the class label.\n",
    "\n",
    "Bayes' Theorem:\n",
    "``` P(y|X) = [P(X|y) * P(y)] / P(X) ```\n",
    "\n",
    "Where:\n",
    "\n",
    "P(y|X): Posterior probability (what we want)\n",
    "\n",
    "P(X|y): Likelihood\n",
    "\n",
    "P(y): Prior probability\n",
    "\n",
    "P(X): Evidence (normalization constant)\n",
    "\n",
    "\n",
    "### The \"Naive\" Assumption\n",
    "\n",
    "The classifier assumes features are independent:\n",
    "\n",
    "\n",
    "``` P(x₁, x₂, ..., xₙ|y) = P(x₁|y) * P(x₂|y) * ... * P(xₙ|y) ```\n",
    "This simplification makes computation feasible but is rarely true in practice (hence \"naive\").\n",
    "\n",
    "\n",
    "Prediction Formula\n",
    "For classification, we predict the class with highest posterior probability:\n",
    "\n",
    "\n",
    "``` ŷ = argmax_y P(y) ∏ P(xᵢ|y) ```\n",
    "\n",
    "## Types of Naive Bayes Classifiers\n",
    "\n",
    "```bash\n",
    "-->>  Gaussian Naive Bayes\n",
    "\n",
    "Use case: Continuous features assumed to follow normal distribution\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "# Probability density function: P(xᵢ|y) = (1/√(2πσ²)) * exp(-(xᵢ-μ)²/(2σ²))\n",
    "\n",
    "\n",
    "\n",
    "-->>  Multinomial Naive Bayes\n",
    "Use case: Discrete counts (text classification, word counts)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB()\n",
    "# Uses smoothed frequency: P(xᵢ|y) = (N_yi + α) / (N_y + α*n)\n",
    "# α (alpha): Smoothing parameter (Laplace smoothing when α=1)\n",
    "\n",
    "\n",
    "-->>  Bernoulli Naive Bayes\n",
    "Use case: Binary/Boolean features (presence/absence)\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bnb = BernoulliNB()\n",
    "# Models binary outcomes: P(xᵢ|y) = P(i|y) if xᵢ=1, else 1-P(i|y)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612f5de1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "feb4475c",
   "metadata": {},
   "source": [
    "Data Preparation Guidelines\n",
    "Text data: Use TF-IDF or CountVectorizer\n",
    "\n",
    "Categorical data: One-hot encoding or label encoding\n",
    "\n",
    "Continuous data: Consider binning for MultinomialNB or use GaussianNB\n",
    "\n",
    "Missing values: Naive Bayes handles missing values naturally in some implementations\n",
    "\n",
    "\n",
    "Handling Common Issues\n",
    "Zero Frequency Problem (When a feature value doesn't appear in a class)\n",
    "\n",
    "Solution: Apply Laplace smoothing (α > 0)\n",
    "\n",
    "Numerical Underflow (Multiplying many small probabilities)\n",
    "\n",
    "Solution: Use log probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f4878b",
   "metadata": {},
   "source": [
    "## Strengths & Weaknesses\n",
    "Advantages\n",
    "Extremely fast training and prediction\n",
    "\n",
    "Works well with high-dimensional data (like text)\n",
    "\n",
    "Requires small amount of training data\n",
    "\n",
    "Handles both continuous and discrete data\n",
    "\n",
    "Simple to implement and interpret\n",
    "\n",
    "Performs well with categorical features\n",
    "\n",
    "Disadvantages\n",
    "Strong independence assumption (rarely true in practice)\n",
    "\n",
    "Zero frequency problem without smoothing\n",
    "\n",
    "Not ideal for regression tasks (primarily for classification)\n",
    "\n",
    "Can be outperformed by more complex models with large datasets\n",
    "\n",
    "Sensitive to irrelevant features (no feature selection built-in)\n",
    "\n",
    "## Real-World Applications\n",
    "1. Text Classification\n",
    "Spam detection (Gmail, Outlook)\n",
    "\n",
    "Sentiment analysis\n",
    "\n",
    "News categorization\n",
    "\n",
    "Language detection\n",
    "\n",
    "2. Medical Diagnosis\n",
    "Disease prediction based on symptoms\n",
    "\n",
    "Risk assessment\n",
    "\n",
    "3. Recommendation Systems\n",
    "Product recommendations\n",
    "\n",
    "Content filtering\n",
    "\n",
    "4. Fraud Detection\n",
    "Credit card fraud\n",
    "\n",
    "Insurance claim fraud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e670b6",
   "metadata": {},
   "source": [
    "## Quick Reference Card\n",
    "When to Use Which Variant\n",
    "GaussianNB: Continuous features, normal distribution assumed\n",
    "\n",
    "MultinomialNB: Discrete counts (word counts, ratings 1-5)\n",
    "\n",
    "BernoulliNB: Binary features (present/absent, true/false)\n",
    "\n",
    "Preprocessing Checklist\n",
    "Handle missing values\n",
    "\n",
    "Encode categorical variables\n",
    "\n",
    "Scale features (for GaussianNB)\n",
    "\n",
    "Apply text vectorization (for text data)\n",
    "\n",
    "Balance classes if needed\n",
    "\n",
    "Common Pitfalls to Avoid\n",
    "Forgetting feature scaling for GaussianNB\n",
    "\n",
    "Not applying smoothing for unseen feature combinations\n",
    "\n",
    "Using raw counts without normalization for text data\n",
    "\n",
    "Ignoring feature correlations when they're important\n",
    "\n",
    "Assuming probabilities are well-calibrated"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
