{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6caecb9",
   "metadata": {},
   "source": [
    "# Gradient Boosting Regressor\n",
    "\n",
    "Gradient Boosting Regressor is an ensemble method that builds many weak decision tree models sequentially, where each new tree tries to fix the errors made by the previous trees.\n",
    "\n",
    "XGBoost\n",
    "\n",
    "LightGBM\n",
    "\n",
    "CatBoost\n",
    "\n",
    "### Common Pitfalls and Solutions\n",
    "\n",
    "Each new tree learns from the \"residuals\" (errors) of the previous tree.\n",
    "\n",
    "Instead of training all trees independently (like Random Forest), Gradient Boosting trains them one after another.\n",
    "\n",
    "\n",
    "Common Pitfalls and Solutions\n",
    "1. Overfitting\n",
    "Symptoms:\n",
    "\n",
    "Training error keeps decreasing while validation error increases\n",
    "\n",
    "Model becomes too complex\n",
    "\n",
    "Solutions:\n",
    "\n",
    "Use smaller learning_rate with more n_estimators\n",
    "\n",
    "Apply stronger regularization (max_depth, min_samples_split)\n",
    "\n",
    "Use subsample < 1.0 (Stochastic GB)\n",
    "\n",
    "Implement early stopping\n",
    "\n",
    "2. Underfitting\n",
    "Symptoms:\n",
    "\n",
    "Both training and validation errors are high\n",
    "\n",
    "Model is too simple\n",
    "\n",
    "Solutions:\n",
    "\n",
    "Increase max_depth\n",
    "\n",
    "Increase n_estimators\n",
    "\n",
    "Increase learning_rate (with caution)\n",
    "\n",
    "Reduce regularization\n",
    "\n",
    "3. Computational Efficiency\n",
    "Optimizations:\n",
    "\n",
    "Use histogram-based boosting (like LightGBM) for large datasets\n",
    "\n",
    "Reduce n_estimators with higher learning_rate\n",
    "\n",
    "Use max_features to limit feature consideration\n",
    "\n",
    " Best Practices\n",
    "Start Simple: Begin with default parameters and iterate\n",
    "\n",
    "Feature Scaling: GBR is generally robust to feature scales, but normalization can help\n",
    "\n",
    "Handle Missing Values: GBR can handle NaNs naturally in some implementations\n",
    "\n",
    "Monitor Learning: Use validation curves to detect overfitting\n",
    "\n",
    "Ensemble Diversity: Consider blending with other models for maximum performance\n",
    "\n",
    "ami ar kico  jani  na  ata  nia \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6d8b4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06546b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load dataset\n",
    "data = fetch_california_housing()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Model\n",
    "model = GradientBoostingRegressor(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"MSE:\", mean_squared_error(Y_test, y_pred))\n",
    "print(\"R2 Score:\", r2_score(Y_test, y_pred))\n",
    "print(\"Feature Importances:\", model.feature_importances_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
