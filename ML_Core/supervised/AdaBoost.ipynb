{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80478b80",
   "metadata": {},
   "source": [
    "# AdaBoost\n",
    "\n",
    "\n",
    "AdaBoost (Adaptive Boosting) is the first practical boosting algorithm that adaptively combines multiple weak classifiers into a strong classifier. It was introduced by Yoav Freund and Robert Schapire in 1997.\n",
    "\n",
    "Core Idea: Sequentially train weak learners, with each new learner focusing more on the examples that previous learners got wrong.\n",
    "\n",
    "2. How AdaBoost Works - Step by Step\n",
    "Initialization:\n",
    "\n",
    "Assign equal weights to all training samples: w₁(i) = 1/N for i = 1,...,N\n",
    "\n",
    "For each round t = 1 to T:\n",
    "\n",
    "Train weak learner h_t on weighted training data\n",
    "\n",
    "Compute weighted error: ε_t = Σ_{i: h_t(x_i) ≠ y_i} w_t(i)\n",
    "\n",
    "Compute learner weight: α_t = 0.5 * ln((1 - ε_t)/ε_t)\n",
    "\n",
    "Update sample weights:\n",
    "\n",
    "Increase weights of misclassified samples: w_{t+1}(i) = w_t(i) * exp(α_t) if wrong\n",
    "\n",
    "Decrease weights of correctly classified: w_{t+1}(i) = w_t(i) * exp(-α_t) if correct\n",
    "\n",
    "Normalize weights: w_{t+1}(i) = w_{t+1}(i) / Σ_j w_{t+1}(j)\n",
    "\n",
    "Final classifier: H(x) = sign(Σ_{t=1}^T α_t * h_t(x))\n",
    "\n",
    "3. Mathematical Foundation\n",
    "AdaBoost minimizes the exponential loss function:\n",
    "\n",
    "\n",
    "``` L(y, F(x)) = exp(-y * F(x))```\n",
    "where F(x) = Σ α_t * h_t(x)\n",
    "The algorithm performs stagewise additive modeling using forward stagewise optimization.\n",
    "\n",
    "## Types of AdaBoost Implementations\n",
    "```bash\n",
    "1. Discrete AdaBoost (SAMME)\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Discrete AdaBoost for binary classification\n",
    "ada_discrete = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=1),  # Decision stump\n",
    "    n_estimators=100,\n",
    "    learning_rate=1.0,\n",
    "    algorithm='SAMME',  # Stagewise Additive Modeling using Multiclass Exponential loss\n",
    "    random_state=42\n",
    ")\n",
    "2. Real AdaBoost (SAMME.R)\n",
    "\n",
    "# Real AdaBoost - uses class probability estimates\n",
    "ada_real = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=3),  # Can use deeper trees\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.5,\n",
    "    algorithm='SAMME.R',  # Requires estimators with predict_proba\n",
    "    random_state=42\n",
    ")\n",
    "3. AdaBoost with Different Base Estimators\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# With SVM (requires probability=True for SAMME.R)\n",
    "ada_svm = AdaBoostClassifier(\n",
    "    estimator=SVC(kernel='linear', probability=True, C=1.0),\n",
    "    n_estimators=50,\n",
    "    learning_rate=0.1,\n",
    "    algorithm='SAMME.R',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# With Naive Bayes\n",
    "ada_nb = AdaBoostClassifier(\n",
    "    estimator=GaussianNB(),\n",
    "    n_estimators=100,\n",
    "    learning_rate=1.0,\n",
    "    algorithm='SAMME',\n",
    "    random_state=42\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3273276",
   "metadata": {},
   "source": [
    "## Key Parameters Explained\n",
    "1. Core AdaBoost Parameters\n",
    "n_estimators: Number of weak learners to train (default=50)\n",
    "\n",
    "Too few: Underfitting, poor performance\n",
    "\n",
    "Too many: Overfitting, increased computation\n",
    "\n",
    "Tip: Monitor validation error, stop when plateaus\n",
    "\n",
    "learning_rate: Shrinks contribution of each classifier (default=1.0)\n",
    "\n",
    "\n",
    "``` α_t = learning_rate * 0.5 * ln((1 - ε_t)/ε_t)  ```\n",
    "Lower learning rate requires more estimators\n",
    "\n",
    "Acts as regularization parameter\n",
    "\n",
    "algorithm: 'SAMME' or 'SAMME.R' (default='SAMME.R')\n",
    "\n",
    "SAMME: Uses discrete predictions, works with any classifier\n",
    "\n",
    "SAMME.R: Uses probability estimates, requires predict_proba\n",
    "```bash\n",
    "2. Base Estimator Parameters\n",
    "\n",
    "# Common base estimators and their key parameters\n",
    "\n",
    "# Decision Tree (most common)\n",
    "base_tree = DecisionTreeClassifier(\n",
    "    max_depth=1,      # Decision stump (most common)\n",
    "    # max_depth=3,    # Slightly stronger weak learner\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Logistic Regression\n",
    "base_lr = LogisticRegression(\n",
    "    C=1.0,\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Support Vector Machine\n",
    "base_svm = SVC(\n",
    "    kernel='linear',\n",
    "    C=1.0,\n",
    "    probability=True,  # Required for SAMME.R\n",
    "    random_state=42\n",
    ")\n",
    "3. Important Attributes After Training\n",
    "\n",
    "# After fitting AdaBoost, you can access:\n",
    "ada = AdaBoostClassifier().fit(X_train, y_train)\n",
    "\n",
    "# Estimator weights (α values)\n",
    "print(\"Estimator weights:\", ada.estimator_weights_)\n",
    "# Shape: (n_estimators,)\n",
    "\n",
    "# Estimator errors (ε values)\n",
    "print(\"Estimator errors:\", ada.estimator_errors_)\n",
    "# Shape: (n_estimators,)\n",
    "\n",
    "# Feature importances\n",
    "print(\"Feature importances:\", ada.feature_importances_)\n",
    "# Shape: (n_features,)\n",
    "\n",
    "# All trained estimators\n",
    "print(\"Number of estimators:\", len(ada.estimators_))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4400d0be",
   "metadata": {},
   "source": [
    "Strengths & Weaknesses\n",
    "Advantages of AdaBoost\n",
    "Simple and Easy to Implement: Few hyperparameters to tune\n",
    "\n",
    "No Prior Knowledge Needed: No assumptions about data distribution\n",
    "\n",
    "Flexible: Can use any weak learner as base estimator\n",
    "\n",
    "Feature Selection: Built-in feature importance\n",
    "\n",
    "Less Overfitting: Compared to single complex models (when properly tuned)\n",
    "\n",
    "Theoretically Grounded: Strong statistical learning foundation\n",
    "\n",
    "Handles Both Binary and Multiclass: With appropriate modifications\n",
    "\n",
    "Disadvantages\n",
    "Sensitive to Noisy Data: Outliers can get high weights\n",
    "\n",
    "Sequential Training: Cannot be parallelized (except for base learner training)\n",
    "\n",
    "Requires Weak Learners: Base estimator should have error < 0.5\n",
    "\n",
    "Can Overfit: With too many estimators or complex base learners\n",
    "\n",
    "Slow on Large Datasets: Due to sequential nature\n",
    "\n",
    "Memory Intensive: Stores all weak learners\n",
    "\n",
    "When to Use AdaBoost\n",
    " Moderate-sized datasets (thousands to tens of thousands of samples)\n",
    "\n",
    " Binary or multiclass classification problems\n",
    "\n",
    " Need interpretable feature importance\n",
    "\n",
    " Want a strong baseline model\n",
    "\n",
    " Data is relatively clean (not too noisy)\n",
    "\n",
    "When to Avoid AdaBoost\n",
    " Very large datasets (consider gradient boosting variants)\n",
    "\n",
    " Noisy data with many outliers\n",
    "\n",
    " Need for parallel training\n",
    "\n",
    " Online/streaming learning requirements\n",
    "\n",
    " Regression problems (use AdaBoostRegressor instead)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6827b7bf",
   "metadata": {},
   "source": [
    "## Real-World Applications\n",
    "Face Detection\n",
    "Customer Churn Prediction\n",
    "Medical Diagnosis\n",
    "Disease detection from medical images\n",
    "\n",
    "Patient risk stratification\n",
    "\n",
    "Diagnostic decision support systems\n",
    "\n",
    "Fraud Detection\n",
    "Credit card fraud detection\n",
    "\n",
    "Insurance claim fraud identification\n",
    "\n",
    "Anomaly detection in transactions\n",
    "\n",
    "Text Classification\n",
    "Spam filtering\n",
    "\n",
    "Sentiment analysis\n",
    "\n",
    "Document categorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189c5b37",
   "metadata": {},
   "source": [
    "hy is AdaBoost called \"Adaptive\" Boosting?\n",
    "\n",
    "The algorithm adapts to the errors of previous weak learners by increasing the weights of misclassified samples. This adaptive reweighting focuses subsequent learners on harder examples, making the ensemble increasingly effective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17419bbd",
   "metadata": {},
   "source": [
    "What's the intuition behind the weight update formula α_t = 0.5 * ln((1-ε_t)/ε_t)?\n",
    "\n",
    "This formula ensures that:\n",
    "*1. More accurate classifiers get higher weight: When ε_t < 0.5, (1-ε_t)/ε_t > 1, so α_t > 0*\n",
    "*2. Perfect classifier gets infinite weight: If ε_t = 0, α_t → ∞*\n",
    "*3. Random classifier gets zero weight: If ε_t = 0.5, α_t = 0*\n",
    "*4. Worse-than-random gets negative weight: If ε_t > 0.5, α_t < 0 (can flip predictions)*\n",
    "\n",
    "Q3: Why are decision stumps (depth=1 trees) commonly used with AdaBoost?\n",
    "\n",
    "1. They're weak learners: Error rate slightly better than random guessing\n",
    "2. Fast to train: Simple structure\n",
    "3. High bias, low variance: Ideal for boosting which reduces bias\n",
    "4. Interpretable: Easy to understand individual decisions\n",
    "5. Theoretical guarantees: AdaBoost provably boosts weak learners\n",
    "\n",
    "Q4: How does AdaBoost handle multiclass classification?\n",
    "\n",
    "Two main approaches:\n",
    "1. SAMME (Stagewise Additive Modeling using Multiclass Exponential loss): Direct extension to multiclass\n",
    "*2. One-vs-Rest or One-vs-One: Decompose into binary problems*\n",
    "*The weight update becomes: α_t = ln((1-ε_t)/ε_t) + ln(K-1) where K is number of classes*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
