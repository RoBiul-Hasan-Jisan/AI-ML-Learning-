{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52f16c30",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c84be5ba",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    " XGBoost stands for eXtreme Gradient Boosting .\n",
    " Gradient Boosting: It belongs to the family of ensemble learning methods, specifically using the Gradient Boosting framework.\n",
    "\n",
    "Ensemble Learning: The core idea is to combine multiple weak learners (typically decision trees) to create a single strong learner. The model \"learns\" from its previous mistakes.\n",
    "\n",
    "Regressor: In the context of regression, the goal is to predict a continuous numerical value (e.g., house price, temperature, sales amount).\n",
    "\n",
    "Analogy: The Student Learning from Mistakes\n",
    "Imagine a student preparing for an exam:\n",
    "\n",
    "First Try (First Tree): The student takes a practice test and gets a score. They analyze their mistakes.\n",
    "\n",
    "Second Try (Second Tree): They focus specifically on the topics they got wrong in the first test.\n",
    "\n",
    "Repeat (Subsequent Trees): They keep taking new tests, each time paying extra attention to the errors made in the previous test.\n",
    "\n",
    "Final Exam (Prediction): For the final exam, the student's knowledge is the sum of all the incremental learning from each practice test.\n",
    "\n",
    "XGBoost formalizes this intuitive process with mathematical rigor.\n",
    "\n",
    "## How it Works\n",
    "\n",
    "Let's break down the process step-by-step.\n",
    "\n",
    "2.1 The High-Level Idea: Sequential Ensemble\n",
    "Unlike Random Forest which builds trees in parallel, XGBoost builds trees sequentially, one after the other. Each new tree is trained to correct the residual errors made by the combination of all previous trees.\n",
    "\n",
    "2.2 The Mathematical Formulation\n",
    "a) The Model\n",
    "The final prediction for a data point i after K trees is:\n",
    "≈∑_i = œÜ(x_i) = Œ£_{k=1}^{K} f_k(x_i)\n",
    "where:\n",
    "\n",
    "≈∑_i is the final predicted value.\n",
    "\n",
    "x_i is the feature vector for data point i.\n",
    "\n",
    "f_k is the prediction from the k-th decision tree.\n",
    "\n",
    "The model is simply the sum of the predictions of all the trees.\n",
    "\n",
    "b) The Objective Function\n",
    "This is the heart of XGBoost. The goal is to find the set of trees {f_k} that minimizes the following objective function:\n",
    "Obj(Œ∏) = Œ£_i L(y_i, ≈∑_i) + Œ£_k Œ©(f_k)\n",
    "\n",
    "This function has two critical parts:\n",
    "\n",
    "Training Loss (L): Measures how well our model fits the training data.\n",
    "\n",
    "For regression, this is often:\n",
    "\n",
    "Squared Loss: L = (y_i - ≈∑_i)¬≤\n",
    "\n",
    "Absolute Loss: L = |y_i - ≈∑_i|\n",
    "\n",
    "Regularization Term (Œ©): Penalizes the complexity of the model to prevent overfitting. This is a key advantage of XGBoost over classic GBM.\n",
    "\n",
    "Œ©(f) = Œ≥T + (1/2)Œª||w||¬≤\n",
    "\n",
    "T: Number of leaves in the tree.\n",
    "\n",
    "w: The score (prediction value) on each leaf.\n",
    "\n",
    "Œ≥ (gamma): Complexity cost for adding a new leaf. A higher Œ≥ makes the algorithm more conservative.\n",
    "\n",
    "Œª (lambda): L2 regularization term on the leaf weights. This shrinks the leaf scores towards zero.\n",
    "\n",
    "2.3 The Learning Process: Additive Training\n",
    "We start with an initial prediction (e.g., the average of the target variable for regression).\n",
    "≈∑_i^(0) = 0\n",
    "\n",
    "Step 1: Build the first tree f_1 to minimize the objective.\n",
    "≈∑_i^(1) = ≈∑_i^(0) + f_1(x_i)\n",
    "\n",
    "Step 2: Build the second tree f_2 to minimize the objective, given the current model ≈∑_i^(1).\n",
    "≈∑_i^(2) = ≈∑_i^(1) + f_2(x_i)\n",
    "\n",
    "Step t: At step t, we add the tree f_t that improves our model the most.\n",
    "≈∑_i^(t) = ≈∑_i^(t-1) + f_t(x_i)\n",
    "\n",
    "The key is that we are greedily adding the tree f_t that best reduces our overall objective function.\n",
    "\n",
    "## How is the Next Tree Built? (Gradient Boosting)\n",
    "\n",
    " We use Gradient Descent in function space. We don't have parameters like in neural networks; our \"parameters\" are the functions (trees) themselves.\n",
    "\n",
    " Compute Gradients: For a given loss function (e.g., Squared Error), we calculate the gradient (g_i) and Hessian (h_i) for each data point i with respect to the previous prediction ≈∑_i^(t-1).\n",
    "\n",
    " for Squared Loss L = (y_i - ≈∑_i)¬≤:\n",
    "\n",
    " g_i = ‚àÇL/‚àÇ≈∑ = -2(y_i - ≈∑_i) (First derivative)\n",
    "\n",
    " h_i = ‚àÇ¬≤L/‚àÇ≈∑¬≤ = 2 (Second derivative)\n",
    "\n",
    " Fit a New Tree: We now fit a new decision tree f_t to predict the negative gradients. In essence, the tree is learning the \"direction\" in which we need to adjust our predictions to reduce the loss.\n",
    "\n",
    " Find the Optimal Leaf Weights: Once the tree structure is built, we need to find the best output value (weight w_j) for each leaf j. We can solve this analytically by setting the derivative of the objective (for that leaf) to zero. The optimal weight for leaf j is:\n",
    " w_j* = - ( Œ£_{i ‚àà I_j} g_i ) / ( Œ£_{i ‚àà I_j} h_i + Œª )\n",
    " where I_j is the set of data points in leaf j.\n",
    "\n",
    " Calculate the Objective Reduction: For a given split in the tree, we can calculate the \"gain\" in the objective function if we were to make that split.\n",
    " Gain = [ (Œ£_{left} g_i)¬≤ / (Œ£_{left} h_i + Œª) + (Œ£_{right} g_i)¬≤ / (Œ£_{right} h_i + Œª) - (Œ£_{parent} g_i)¬≤ / (Œ£_{parent} h_i + Œª) ] / 2 - Œ≥\n",
    "\n",
    " The algorithm uses this exact formula to decide the best split! It will choose the split that gives the highest Gain. Notice how Œ≥ is subtracted at the end‚Äîthis means the split must improve the loss by at least Œ≥ to be considered.\n",
    "\n",
    " ```bash \n",
    " XGBoost is an ensemble model\n",
    "\n",
    " It builds many decision trees, but:\n",
    "\n",
    " Trees are not independent (unlike Random Forest)\n",
    "\n",
    " They are built sequentially\n",
    "\n",
    " Each new tree learns the errors (residuals) of previous trees\n",
    "\n",
    " This technique is called Gradient Boosting.\n",
    "\n",
    " Goal\n",
    "\n",
    " Predict a continuous value ‚Üí Regression \n",
    " ```\n",
    " ---\n",
    "\n",
    "### XGBoost adds:\n",
    "```bash\n",
    "Feature                      Meaning\n",
    "\n",
    "Regularization (L1 & L2)    Avoids overfitting\n",
    "\n",
    "Parallel tree building\t    Uses all CPU cores\n",
    "\n",
    "Handling missing values\t    Automatically splits missing data\n",
    "\n",
    "Tree pruning\t            Removes useless branches\n",
    "\n",
    "Weighted quantile sketch\tFast handling of large data\n",
    "\n",
    "Shrinkage (learning_rate)\tPrevents overfitting\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "# Use\n",
    "Target (y) is continuous\n",
    "\n",
    "Features (X) are numeric\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192df720",
   "metadata": {},
   "source": [
    "## XGBoost Regressor vs Classifier \n",
    "\n",
    "## Problem Type & Output\n",
    "``` bash\n",
    "XGBoost Classifier\t                 XGBoost Regressor\n",
    "Classification problems\t                  Regression problems\n",
    "Predicts class labels or                  Predicts continuous\n",
    "    probabilities\t    \n",
    "\n",
    "Output: 0/1, or [0.3, 0.7] (probabilities)  Output: 125.7, -2.45\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "``` bash\n",
    "2. Objectives & Loss Functions\n",
    "Classifier\t                   Regressor\n",
    "binary:logistic (binary)\t    reg:squarederror (default)\n",
    "multi:softmax (multiclass)\t    reg:linear (L2 loss)\n",
    "multi:softprob (probabilities)\t    reg:gamma (Gamma regression)\n",
    "For probabilities\t            For continuous values\n",
    "\n",
    "``` \n",
    "\n",
    "\n",
    "## Use XGBoost Classifier when:\n",
    "Predicting categories (spam/ham, sick/healthy, fraud/not fraud)\n",
    "\n",
    "Need probability outputs (predict_proba())\n",
    "\n",
    "Working with classification metrics (accuracy, precision, recall, AUC)\n",
    "\n",
    "Binary or multiclass classification problems\n",
    "\n",
    "## Use XGBoost Regressor when:\n",
    "Predicting continuous numerical values\n",
    "\n",
    "Forecasting quantities (sales, prices, temperatures)\n",
    "\n",
    "Working with regression metrics (RMSE, MAE, R¬≤)\n",
    "\n",
    "Any regression problem with tabular data\n",
    "\n",
    "\n",
    "\n",
    "## Key Differences\n",
    "\n",
    "Target Type:\n",
    "\n",
    "Regressor ‚Üí numeric\n",
    "\n",
    "Classifier ‚Üí categorical\n",
    "\n",
    "Output Transformation:\n",
    "\n",
    "Regressor ‚Üí raw number\n",
    "\n",
    "Classifier ‚Üí probability (sigmoid for binary, softmax for multi-class)\n",
    "\n",
    "Gradient & Hessian:\n",
    "\n",
    "Regressor ‚Üí simple difference: grad = pred - target\n",
    "\n",
    "Classifier ‚Üí computed using derivative of logloss\n",
    "\n",
    "Loss Function:\n",
    "\n",
    "Regressor ‚Üí MSE, MAE\n",
    "\n",
    "Classifier ‚Üí Logloss / Cross-Entropy\n",
    "\n",
    "\n",
    "- XGBoost Regressor ‚Üí numeric prediction, regression loss\n",
    "\n",
    "- XGBoost Classifier ‚Üí probability prediction, classification loss\n",
    "\n",
    "- Core tree-building and boosting framework ‚Üí identical\n",
    "\n",
    "- To convert regressor ‚Üí classifier, just change loss + gradient/     hessian + output mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e11cec60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective Function (Squared Error)\n",
    "def squared_error_grad_hess(y_true,y_pred):\n",
    "    # gradient: d/dy_pred (1/2 * (y - y_pred)^2) = y_pred - y\n",
    "    g = y_pred - y_true\n",
    "    # hessian: second derivative = 1\n",
    "    h = 1.0 \n",
    "    return g,h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38478963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure of a Tree Node\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value  # leaf weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b09b65",
   "metadata": {},
   "source": [
    "Compute Leaf Weight (XGBoost Formula)\n",
    "ùë§  = ‚àí ùê∫/ùêª+ùúÜ\n",
    "‚Äã\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6360dd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_leaf_weight(G, H, lambda_):\n",
    "    return - G / (H + lambda_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df8026b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gain of a Split\n",
    "def compute_gain(GL, HL, GR, HR, lambda_, gamma):\n",
    "    parent_gain = (GL + GR)**2 / (HL + HR + lambda_)\n",
    "    left_gain = GL**2 / (HL + lambda_)\n",
    "    right_gain = GR**2 / (HR + lambda_)\n",
    "    return 0.5 * (left_gain + right_gain - parent_gain) - gamma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7fa120e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "#  OBJECTIVE (Squared Error)\n",
    "\n",
    "def squared_error_grad_hess(y_true, y_pred):\n",
    "    g = y_pred - y_true  # gradient\n",
    "    h = np.ones_like(y_true)  # hessian (constant)\n",
    "    return g, h\n",
    "\n",
    "\n",
    "#  TREE NODE\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value  # leaf weight\n",
    "\n",
    "\n",
    "#  LEAF WEIGHT FORMULA\n",
    "#  w = -G / (H + Œª)\n",
    "\n",
    "def compute_leaf_weight(G, H, lambda_):\n",
    "    return -G / (H + lambda_)\n",
    "\n",
    "\n",
    "#  XGBOOST GAIN FORMULA\n",
    "\n",
    "def compute_gain(GL, HL, GR, HR, lambda_, gamma):\n",
    "    parent = (GL + GR)**2 / (HL + HR + lambda_)\n",
    "    left = GL**2 / (HL + lambda_)\n",
    "    right = GR**2 / (HR + lambda_)\n",
    "    return 0.5 * (left + right - parent) - gamma\n",
    "\n",
    "\n",
    "#  FIND BEST SPLIT\n",
    "\n",
    "def find_best_split(X, g, h, lambda_, gamma):\n",
    "    best_gain = -float(\"inf\")\n",
    "    best_feature = None\n",
    "    best_threshold = None\n",
    "\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    for feature in range(n_features):\n",
    "        thresholds = np.unique(X[:, feature])\n",
    "\n",
    "        for t in thresholds:\n",
    "            left_idx = X[:, feature] <= t\n",
    "            right_idx = ~left_idx\n",
    "\n",
    "            if left_idx.sum() == 0 or right_idx.sum() == 0:\n",
    "                continue\n",
    "\n",
    "            GL, HL = g[left_idx].sum(), h[left_idx].sum()\n",
    "            GR, HR = g[right_idx].sum(), h[right_idx].sum()\n",
    "\n",
    "            gain = compute_gain(GL, HL, GR, HR, lambda_, gamma)\n",
    "\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_feature = feature\n",
    "                best_threshold = t\n",
    "                \n",
    "    return best_feature, best_threshold, best_gain\n",
    "\n",
    "\n",
    "#  BUILD TREE (Recursive)\n",
    "\n",
    "def build_tree(X, g, h, depth, max_depth, lambda_, gamma):\n",
    "    if depth == max_depth:\n",
    "        return Node(value=compute_leaf_weight(g.sum(), h.sum(), lambda_))\n",
    "\n",
    "    feature, threshold, gain = find_best_split(X, g, h, lambda_, gamma)\n",
    "\n",
    "    if (feature is None) or (gain < 0):\n",
    "        return Node(value=compute_leaf_weight(g.sum(), h.sum(), lambda_))\n",
    "\n",
    "    left_idx = X[:, feature] <= threshold\n",
    "    right_idx = ~left_idx\n",
    "\n",
    "    left_child = build_tree(X[left_idx], g[left_idx], h[left_idx], depth+1, max_depth, lambda_, gamma)\n",
    "    right_child = build_tree(X[right_idx], g[right_idx], h[right_idx], depth+1, max_depth, lambda_, gamma)\n",
    "\n",
    "    return Node(feature, threshold, left_child, right_child)\n",
    "\n",
    "\n",
    "#  PREDICT WITH ONE TREE\n",
    "\n",
    "def predict_tree(node, x):\n",
    "    if node.value is not None:\n",
    "        return node.value\n",
    "\n",
    "    if x[node.feature] <= node.threshold:\n",
    "        return predict_tree(node.left, x)\n",
    "    else:\n",
    "        return predict_tree(node.right, x)\n",
    "\n",
    "\n",
    "#  XGBOOST REGRESSOR CORE\n",
    "\n",
    "class XGBoostRegressorCore:\n",
    "    def __init__(self, n_estimators=50, learning_rate=0.1, max_depth=3, lambda_=1, gamma=0):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.lambda_ = lambda_\n",
    "        self.gamma = gamma\n",
    "        self.trees = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n = len(y)\n",
    "        y_pred = np.zeros(n)\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            g = y_pred - y  # gradient\n",
    "            h = np.ones(n)  # hessian\n",
    "\n",
    "            tree = build_tree(X, g, h, 0, self.max_depth, self.lambda_, self.gamma)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "            update = np.array([predict_tree(tree, x) for x in X])\n",
    "            y_pred += self.learning_rate * update\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = np.zeros(X.shape[0])\n",
    "\n",
    "        for tree in self.trees:\n",
    "            preds += self.learning_rate * np.array([predict_tree(tree, x) for x in X])\n",
    "\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a537c112",
   "metadata": {},
   "source": [
    "XGBoost is a gradient boosting algorithm:\n",
    "\n",
    "Start with all predictions = 0\n",
    "\n",
    "Compute gradient (how wrong the model is)\n",
    "\n",
    "Build a decision tree that predicts the gradient\n",
    "\n",
    "Add this tree to the model\n",
    "\n",
    "Repeat for many trees\n",
    "\n",
    "Each tree fixes the errors of the previous trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0164d84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we can do this (Classifier):\n",
    "from xgboost import XGBClassifier\n",
    "model_clf = XGBClassifier(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "# Then you automatically know this (Regressor):\n",
    "from xgboost import XGBRegressor\n",
    "model_reg = XGBRegressor(\n",
    "    n_estimators=1000,           # Same\n",
    "    learning_rate=0.05,          # Same  \n",
    "    max_depth=6,                 # Same\n",
    "    objective='reg:squarederror', # Only this changes!\n",
    "    eval_metric='rmse'           # And this changes\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
