{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ad8cca9",
   "metadata": {},
   "source": [
    "# KNN\n",
    "KNN is a supervised, instance-based (lazy learner), and non-parametric algorithm used for classification and regression. It makes predictions by finding the *k* closest data points (neighbors) in the training set to a new input and using a majority vote (classification) or an average (regression) of their values.\n",
    "\n",
    "The core steps for making a prediction on a new data point are:\n",
    "\n",
    "Calculate Distance: Measure the distance (e.g., Euclidean, Manhattan) from the new point to all points in the training set.\n",
    "\n",
    "Find Neighbors: Identify the *k* points with the smallest distances.\n",
    "\n",
    "Aggregate for Output:\n",
    "\n",
    "For classification: Assign the most common class among the *k* neighbors.\n",
    "\n",
    "For regression: Assign the average value of the *k* neighbors.\n",
    "\n",
    "\n",
    "## Critical Parameters and Hyperparameter Tuning\n",
    "The performance of KNN hinges on several key decisions.\n",
    "\n",
    "Choosing 'k' (Number of Neighbors): This is the most critical parameter. A small *k* (like 1) makes the model sensitive to noise, while a very large *k* oversimplifies the model. An odd *k* value is preferred for classification to avoid ties. The optimal *k* is data-dependent and is typically found using cross-validation or the Elbow Method.\n",
    "\n",
    "Selecting a Distance Metric: The choice defines \"closeness.\" Common metrics include:\n",
    "\n",
    "Euclidean Distance: The straight-line distance (default for continuous features).\n",
    "\n",
    "Manhattan Distance: Sum of absolute differences (useful for grid-like data).\n",
    "\n",
    "Minkowski Distance: A generalized formula; setting p=2 gives Euclidean, p=1 gives Manhattan.\n",
    "\n",
    "## Other Tuning Parameters:\n",
    "\n",
    "Weights: Neighbors can be weighted uniformly or by the inverse of their distance (weights='distance'), giving closer points more influence.\n",
    "\n",
    "Algorithm: Methods like 'ball_tree' or 'kd_tree' can be faster than 'brute' force for larger datasets.\n",
    "\n",
    "## Best Practice: Use GridSearchCV or RandomizedSearchCV from scikit-learn to systematically test combinations of these parameters (e.g., n_neighbors, weights, metric) and find the best set via cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16345e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import Libraries & Load Data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "# X = df.drop('target_column', axis=1)\n",
    "# y = df['target_column']\n",
    "\n",
    "# 2. Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Feature Scaling (CRITICAL for KNN)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 4. Hyperparameter Tuning with Grid Search\n",
    "param_grid = {\n",
    "    'n_neighbors': list(range(1, 31)),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "knn = KNeighborsClassifier()\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 5. Evaluate Best Model\n",
    "best_knn = grid_search.best_estimator_\n",
    "y_pred = best_knn.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Best Params: {grid_search.best_params_}, Test Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f01837b",
   "metadata": {},
   "source": [
    "Use KNN when:\n",
    "\n",
    "You have a small to moderately sized dataset.\n",
    "\n",
    "Interpretability is important (explaining \"nearest neighbors\" is intuitive).\n",
    "\n",
    "Data has a non-linear pattern and you need a simple baseline model.\n",
    "\n",
    "Avoid KNN when:\n",
    "\n",
    "The dataset is very large (slow prediction).\n",
    "\n",
    "The dataset has many features (high-dimensional).\n",
    "\n",
    "Prediction speed is a critical requirement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc108bc",
   "metadata": {},
   "source": [
    "Real-World Applications\n",
    "Due to its intuitive logic, KNN is widely used in:\n",
    "\n",
    "Recommendation Systems: Finding users with similar tastes to suggest products or content.\n",
    "\n",
    "Pattern Recognition & Security: Detecting fraudulent credit card transactions by identifying anomalous patterns.\n",
    "\n",
    "Healthcare: Classifying medical diagnoses, such as predicting the risk of a disease based on similar patient records.\n",
    "\n",
    "Finance: Credit scoring, stock market forecasting, and customer profiling"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
