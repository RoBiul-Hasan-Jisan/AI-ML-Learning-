{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29140e91",
   "metadata": {},
   "source": [
    "# LightGBM\n",
    "LightGBM stands for Light Gradient Boosting Machine. It is a gradient boosting framework developed by Microsoft that is designed to be:\n",
    "\n",
    "Faster than other GBM implementations \n",
    "\n",
    "More memory efficient\n",
    "\n",
    "Able to handle large-scale data\n",
    "\n",
    "Like XGBoost, it's an ensemble method that uses decision trees, but it employs several novel techniques to achieve its performance advantages.\n",
    "\n",
    "### Key Philosophy: Why be \"Light\"?\n",
    "Traditional boosting algorithms (including XGBoost's default) grow trees level-wise (horizontally). LightGBM grows trees leaf-wise (vertically), which is more efficient but can lead to overfitting on small datasets if not properly regularized.\n",
    "\n",
    "### How it Works\n",
    "LightGBM shares the same fundamental gradient boosting framework with XGBoost. The objective function is similar:\n",
    "Obj(θ) = Σ_i L(y_i, ŷ_i) + Σ_k Ω(f_k)\n",
    "\n",
    "Where the regularization term Ω(f_k) also penalizes the number of leaves and the L2 norm of leaf weights.\n",
    "\n",
    "However, its key differentiators lie in HOW it builds the trees.\n",
    "\n",
    "Gradient-Based One-Side Sampling (GOSS)\n",
    "Idea: Not all data points are equally important for boosting. Data points with larger gradients (i.e., those that are poorly predicted) contribute more to the information gain.\n",
    "\n",
    "### How it works:\n",
    "\n",
    "Sort the training instances by the absolute value of their gradients.\n",
    "\n",
    "Keep the top a% of instances with the largest gradients.\n",
    "\n",
    "Randomly sample b% from the remaining instances (those with small gradients).\n",
    "\n",
    "When computing the gain for splits, GOSS amplifies the weight of the sampled data with small gradients by a constant factor (1-a)/b.\n",
    "\n",
    "This allows LightGBM to focus computational resources on the \"difficult\" examples while still maintaining the original data distribution, leading to faster training with minimal accuracy loss.\n",
    "\n",
    "## Exclusive Feature Bundling (EFB)\n",
    "Idea: In high-dimensional data, many features are sparse and often mutually exclusive (they never take non-zero values simultaneously). EFB bundles these features together to reduce the dimensionality.\n",
    "\n",
    "How it works:\n",
    "\n",
    "Identify Exclusive Features: Find features that rarely take non-zero values simultaneously.\n",
    "\n",
    "Bundle Them: Merge these features into a single \"bundled\" feature.\n",
    "\n",
    "Result: The number of features is significantly reduced, speeding up the training process without losing much information.\n",
    "\n",
    "This is particularly effective for one-hot encoded categorical features.\n",
    "\n",
    "## Leaf-Wise (Best-First) Tree Growth\n",
    "This is the most significant difference from XGBoost's default approach.\n",
    "\n",
    "XGBoost (Level-Wise): Grows the tree level by level. At each level, all leaves are split simultaneously. This can be inefficient as it may split even leaves that contribute little to the loss reduction.\n",
    "\n",
    "LightGBM (Leaf-Wise): At each step, it identifies the leaf that will yield the largest reduction in the loss and splits only that leaf. This results in much deeper trees for the same number of leaves and often achieves lower loss.\n",
    "\n",
    "Trade-off: Leaf-wise growth is more prone to overfitting, especially on small datasets, which is why LightGBM has important regularization parameters like num_leaves and min_data_in_leaf.\n",
    "\n",
    "\n",
    "## LightGBM\t               \n",
    "Tree Growth\t         Leaf-wise\t \n",
    "\n",
    "Speed\t             Faster\t  \n",
    "\n",
    "Memory Usage         Lower\n",
    "\n",
    "Handling Large Data\t  Excellent\n",
    "\n",
    "Categorical Features    Native support (no need for one-hot encoding)\n",
    "## XGBoost\t\n",
    "                         \n",
    "Tree Growth\t  \t            Higher \n",
    "\n",
    "Speed\t       \t          Level-wise\n",
    "\n",
    "Memory Usage               Slower\n",
    "\n",
    "Handling Large Data        Good \n",
    "\n",
    "Categorical Features      Requires one-hot encoding or preprocessing\n",
    "\n",
    "\n",
    "# When to Choose LightGBM vs XGBoost\n",
    "\n",
    "## Choose LightGBM when:\n",
    "\n",
    "You have very large datasets (millions of rows)\n",
    "\n",
    "Training time is critical\n",
    "\n",
    "You have limited computational resources (memory)\n",
    "\n",
    "Your data has many categorical features\n",
    "\n",
    "You're dealing with high-dimensional data\n",
    "\n",
    "## Choose XGBoost when:\n",
    "\n",
    "Your dataset is small to medium-sized\n",
    "\n",
    "You want maximum performance and are willing to wait\n",
    "\n",
    "You need extremely robust results (less prone to overfitting on small data)\n",
    "\n",
    "You want fine-grained control over the training process\n",
    "\n",
    "```bash\n",
    "\n",
    "Leaf-Wise Tree Growth (Important)\n",
    "\n",
    "XGBoost grows level-wise\n",
    "\n",
    "LightGBM grows leaf-wise (chooses the best leaf to split)\n",
    "\n",
    "\n",
    "         Root\n",
    "       /      \\\n",
    "   Leaf A     Leaf B  <- LightGBM splits where maximum gain is\n",
    "\n",
    "\n",
    "   This gives:\n",
    "\n",
    " Better accuracy\n",
    " Deeper useful trees\n",
    "\n",
    "But risk:\n",
    "Overfitting — solved using max_depth\n",
    "\n",
    "\n",
    "```\n",
    "- CatBoost Regressor → numeric output\n",
    "\n",
    "- CatBoost Classifier → class probability output\n",
    "\n",
    "- Core boosting logic is almost identical\n",
    "\n",
    "- Only loss function and output mapping changes\n",
    "\n",
    "## Key Difference\n",
    "\n",
    "In regression, CatBoost predicts numbers.\n",
    "\n",
    "In classification, CatBoost predicts probabilities (0 to 1 for binary, or softmax for multi-class).\n",
    "\n",
    "Internally, the tree structure, ordered boosting, and categorical handling are almost identical.\n",
    "\n",
    "Only the loss function and gradient calculation change between Regressor and Classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7202cc0c",
   "metadata": {},
   "source": [
    "# LightGBM Regressor vs Classifier \n",
    "## Problem Type & Output\n",
    "```bash\n",
    "\n",
    "LightGBM Classifier\t            LightGBM Regressor\n",
    "Classification problems\t        Regression problems\n",
    "Predicts class labels or        Predicts continuous values\n",
    "          probabilities\t\n",
    "Output: 0/1, or [0.3, 0.7]   \tOutput: 125.7, -2.45\n",
    "```\n",
    "## Objectives & Metrics\n",
    "```bash\n",
    "Classifier\t                      Regressor\n",
    "binary (binary classification)\t  regression (L2 loss)\n",
    "multiclass\t                      regression_l1 (MAE)\n",
    "cross_entropy\t                  huber (Huber loss)\n",
    "fair (Fair loss)\t              fair (Fair loss)\n",
    "eval_metric: auc, binary_logloss, eval_metric: l2, l1, rmse, mape\n",
    "     error\n",
    "```\n",
    "\n",
    "\n",
    "## Use LightGBM Classifier when:\n",
    "Predicting categories (yes/no, spam/not spam, multiple classes)\n",
    "\n",
    "Need probability outputs\n",
    "\n",
    "Working with classification metrics (accuracy, precision, AUC)\n",
    "\n",
    "## Use LightGBM Regressor when:\n",
    "Predicting continuous numerical values\n",
    "\n",
    "Forecasting quantities (sales, prices, temperatures)\n",
    "\n",
    "Working with regression metrics (RMSE, MAE, R²)\n",
    "\n",
    "\n",
    "## Key Difference\n",
    "\n",
    "In regression, LightGBM predicts numbers directly.\n",
    "\n",
    "In classification, LightGBM predicts probabilities using sigmoid (binary) or softmax (multi-class).\n",
    "\n",
    "Tree-building, leaf-wise growth, histogram binning, categorical handling remain the same.\n",
    "\n",
    "Only loss function and output mapping differ.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- LightGBM Regressor → numeric output\n",
    "\n",
    "- LightGBM Classifier → probability output\n",
    "\n",
    "- Core tree-building logic is identical\n",
    "\n",
    "- Only loss function and output transformation differ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "046df962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class LightGBMTree:\n",
    "    def __init__(self, max_depth=3, min_samples_leaf=20, num_bins=16):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.num_bins = num_bins\n",
    "        self.tree = None\n",
    "\n",
    "    def _bin_data(self, X):\n",
    "        bins = []\n",
    "        X_binned = np.zeros_like(X, dtype=int)\n",
    "\n",
    "        for j in range(X.shape[1]):\n",
    "            col = X[:, j]\n",
    "            edges = np.linspace(col.min(), col.max(), self.num_bins + 1)\n",
    "            X_binned[:, j] = np.digitize(col, edges) - 1\n",
    "            bins.append(edges)\n",
    "\n",
    "        return X_binned, bins\n",
    "\n",
    "    def _best_split(self, X, grad, hess):\n",
    "        best_gain = -1\n",
    "        best_feature = None\n",
    "        best_bin = None\n",
    "\n",
    "        for j in range(X.shape[1]):\n",
    "            for b in range(self.num_bins):\n",
    "                left = (X[:, j] <= b)\n",
    "                right = ~left\n",
    "\n",
    "                if left.sum() < self.min_samples_leaf or right.sum() < self.min_samples_leaf:\n",
    "                    continue\n",
    "\n",
    "                G_left, H_left = grad[left].sum(), hess[left].sum()\n",
    "                G_right, H_right = grad[right].sum(), hess[right].sum()\n",
    "\n",
    "                gain = (G_left**2 / (H_left + 1e-6)) + (G_right**2 / (H_right + 1e-6))\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = j\n",
    "                    best_bin = b\n",
    "\n",
    "        return best_feature, best_bin\n",
    "\n",
    "    def _build(self, X, grad, hess, depth):\n",
    "        if depth == self.max_depth:\n",
    "            return {\"leaf\": -grad.sum() / (hess.sum() + 1e-6)}\n",
    "\n",
    "        feature, split_bin = self._best_split(X, grad, hess)\n",
    "\n",
    "        if feature is None:\n",
    "            return {\"leaf\": -grad.sum() / (hess.sum() + 1e-6)}\n",
    "\n",
    "        left = (X[:, feature] <= split_bin)\n",
    "        right = ~left\n",
    "\n",
    "        return {\n",
    "            \"feature\": feature,\n",
    "            \"bin\": split_bin,\n",
    "            \"left\": self._build(X[left], grad[left], hess[left], depth + 1),\n",
    "            \"right\": self._build(X[right], grad[right], hess[right], depth + 1),\n",
    "        }\n",
    "\n",
    "    def fit(self, X, grad, hess):\n",
    "        X_binned, self.bins = self._bin_data(X)\n",
    "        self.tree = self._build(X_binned, grad, hess, depth=0)\n",
    "\n",
    "    def _predict_row(self, row, node):\n",
    "        if \"leaf\" in node:\n",
    "            return node[\"leaf\"]\n",
    "\n",
    "        feature = node[\"feature\"]\n",
    "        bin_value = np.digitize(row[feature], self.bins[feature]) - 1\n",
    "\n",
    "        if bin_value <= node[\"bin\"]:\n",
    "            return self._predict_row(row, node[\"left\"])\n",
    "        else:\n",
    "            return self._predict_row(row, node[\"right\"])\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_row(x, self.tree) for x in X])\n",
    "\n",
    "\n",
    "\n",
    "# Gradient Boosting using the custom LightGBM tree\n",
    "\n",
    "\n",
    "class LightGBMRegressorCore:\n",
    "    def __init__(self, n_estimators=10, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        pred = np.zeros_like(y, dtype=float)\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            grad = pred - y\n",
    "            hess = np.ones_like(y)\n",
    "\n",
    "            tree = LightGBMTree(max_depth=self.max_depth)\n",
    "            tree.fit(X, grad, hess)\n",
    "\n",
    "            update = tree.predict(X)\n",
    "            pred -= self.learning_rate * update\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        pred = np.zeros(X.shape[0])\n",
    "\n",
    "        for tree in self.trees:\n",
    "            pred -= self.learning_rate * tree.predict(X)\n",
    "\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf7fc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we can do this (Classifier):\n",
    "from lightgbm import LGBMClassifier\n",
    "model_clf = LGBMClassifier(\n",
    "    n_estimators=1000,\n",
    "    num_leaves=31,\n",
    "    learning_rate=0.05,\n",
    "    categorical_feature=['category_col'],\n",
    "    metric='auc'\n",
    ")\n",
    "\n",
    "# Then you automatically know this (Regressor):\n",
    "from lightgbm import LGBMRegressor\n",
    "model_reg = LGBMRegressor(\n",
    "    n_estimators=1000,           # Same\n",
    "    num_leaves=31,               # Same  \n",
    "    learning_rate=0.05,          # Same\n",
    "    categorical_feature=['category_col'],  # Same\n",
    "    metric='rmse'                # Only this changes!\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
